---
title: "DDSCaseStudy2"
author: "Benjamin Wilke, Manny Rosales, Amy Paschal"
date: "April 2, 2018"
output:
  html_document:
    keep_md: true
---
```{r setup, include=FALSE}
# Load necessary packages and ensure they are active
load.lib = c("kableExtra","ggplot2","Amelia","pastecs","ROCR","reshape2","devtools","glmnet")

install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
  install.packages(lib,dependences=TRUE)
} 

ggbi <- "ggbiplot"
if (!ggbi %in% installed.packages()){ # Not on CRAN, must install via GitHub using devtools package
} 

load.lib = c(load.lib,"ggbiplot")
sapply(load.lib,require,character=TRUE)

knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(kableExtra) # to make uber sexy tables for output
library(ggbiplot) # implementation of biplot using ggplot2 for plotting PCs
library(pastecs) # for easy descriptive statistics
library(ROCR) # for ROC plots and AUC calculations
library(glmnet)
# turn off scientfic notation for the entire script
options(scipen = 999, digits=2)
```

## Introduction

```{r, echo=TRUE}
# load raw data
employeeDatRaw <- read.csv('CaseStudy2data.csv', header=TRUE)
```

# Missing Values

Fortunately, this data is not missing any values as demonstrated by the table below.

```{r, echo=TRUE}
# calculate NAs for raw data
NAdataframe <- sapply(employeeDatRaw, function(x) sum(is.na(x)))
# display NAs in kable table
knitr::kable(NAdataframe, caption = "Counts of NAs (Missing Values) in Raw Data", row.names = TRUE, "html") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"), full_width=FALSE, position = "left")
```

## Data Cleanup and Conversion

#### Checking for missing data values . . .

```{r, echo=TRUE}
# check for missing data values
numNAs <- sum(apply(employeeDatRaw,2,is.na))
```

##### Number of missing data values = `r numNAs`

#### Columns with no variation have no impact on attrition. Dropping . . .

```{r, echo=TRUE}

# look for and drop columns with no variation
drop_columns <- which(apply(employeeDatRaw, 2, function(x) (length(unique(x)) == 1)))
cols <- names(drop_columns)
employeeDatRaw <- employeeDatRaw[,-drop_columns]

# find columns of class factor
factor_columns <- names(which(sapply(names(employeeDatRaw),function(x) class(employeeDatRaw[[x]])=="factor")))

# convert factors to numeric
employeeDatRaw$Attrition <- as.numeric(employeeDatRaw$Attrition)-1
```

##### Dropped columns: `r cols`
##### Factors converted to numeric: `r factor_columns`

## Attrition Rates (Competitor Analysis)

```{r, echo=TRUE}

# determine overall attrition rate
attritionRate <- (sum(employeeDatRaw$Attrition) / nrow(employeeDatRaw)) * 100

ind <- c("Overall","Healthcare","Manufacturing")
vol <- c(13.5, 15.9, 11.1)
total <- c(18.5, 20.5, 17.0)
industryRates <- data.frame(ind,vol,total)
names(industryRates) <- c("Industry", "Voluntary(%)", "Total(%)")

# display industry attrition
knitr::kable(industryRates, caption = "Attrition Rates. Per Compdata Surveys & Consulting's Turnover Report 2017", row.names = FALSE, "html") %>%
  kable_styling(bootstrap_options = c("striped","hover", "condensed", "responsive"))
```
##### reference: http://blog.compdatasurveys.com/employee-turnover-trends-in-2017

## Attrition Rates (XYZ Company)

##### Overall attrition rate: `r attritionRate`%

```{r, echo=TRUE, results="asis"}

#############################################################
# Determine and display attrition rates by attribute.
#############################################################

# AgeRange doesn't exist so it needs to be created.
# AgeRange is used for statistical binning of ages.

# generate age range bin
ageBin <- function(x) {
  if (x < 25) return ("< 25")
  if (x < 35) return ("25-35")
  if (x < 45) return ("35-45")
  if (x < 55) return ("45-55")
  return("55 and >")
}

attritionRaw <- employeeDatRaw
attritionRaw$AgeRange <- sapply(attritionRaw$Age, ageBin)

# attributes of interest
# to change the set of attributes for which attrition rates are calculated,
# simply change the next line
attritionAttrs <- c("Department", "JobRole", "Gender", "AgeRange")

###########################################################
# helper functions for generating and displaying data
###########################################################

# generateAttritionDF generates a dataframe for attrition by attr, where attr is an
# attribute (aka column) of the attrition dataframe
generateAttritionDF <- function(attr,df) {
  # construct the "by" list for the aggregate function
  attrList <- list(df[[attr]])
  names(attrList)[1] <- attr

  # aggreate by attribute, counting the number of observations where attrition is true
  attritionByAttr <- aggregate(df$Attrition,by=attrList,FUN=sum)
  # determine number of observations for each attribute value
  sizeByAttr <- count(df,attr)
  # merge into a new dataframe and calculate the attrition rate for each attribute value
  attritionRateByAttr <- merge(attritionByAttr, sizeByAttr, by=attr)
  attritionRateByAttr$Rate <- (attritionRateByAttr$x / attritionRateByAttr$freq) * 100
  names(attritionRateByAttr) <- c(attr, "Attrition", "PopulationSize", "AttritionRate")
  return(attritionRateByAttr)
}

# displayAttritionAttr displays the given data frame in a table with an appropriate title
displayAttritionAttr <- function(attr, df) {
    title <- paste("Attrition Rates by ", attr)
    print(knitr::kable(df, caption = title, row.names = FALSE, "html") %>% kable_styling(bootstrap_options = c("striped","hover", "condensed", "responsive")))
    return
}

#########################################################
# end helper functions
#########################################################

# generate a table and display it for each attribute (column) of interest
for (attr in attritionAttrs) {
  displayAttritionAttr(attr, generateAttritionDF(attr, attritionRaw))
}
```
## Additional Information to Collect
##### Voluntary/involuntary breakdown
##### Better job/department breakdown for Research & Development

# Exploratory Data Analysis
``` {r MannyEDA, echo=TRUE,fig.height = 16, fig.width = 10, fig.align="center"}
# descriptive statistics, load into new data frame for processing
descriptiveTable <- pastecs::stat.desc(employeeDatRaw)

# remove non-numeric features
charCols <- c("Attrition", "BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "Over18", "OverTime")
continuousTable.stats <- descriptiveTable[,!(colnames(descriptiveTable) %in% charCols)]
categoricalTable <- employeeDatRaw[,(colnames(descriptiveTable) %in% charCols)]
continuousTable <- employeeDatRaw[,!(colnames(employeeDatRaw) %in% charCols)]

# remove rows for certain descriptive statistics leaving: N, Mean, Median, Std Dev, Var, Min, Max
remove <- c("CI.mean.0.95", "nbr.val", "nbr.null", "nbr.na", "range", "sum", "SE.mean", "CI.mean", "coef.var")
continuousTable.stats <- continuousTable.stats[-which(rownames(continuousTable.stats) %in% remove),]

# round all numeric values to 2 decimal points
continuousTable.stats <- round(continuousTable.stats, 2)
continuousTable.transposed <- t(continuousTable.stats) # object becomes matrix

# display descriptive statistics
knitr::kable(continuousTable.transposed,caption = "Descriptive Statistics for Numeric Features in the Raw Employee Data", row.names = TRUE, "html") %>%
  kable_styling(bootstrap_options = c("striped","hover", "condensed", "responsive"), full_width = F)

summary(categoricalTable)

head(melt(continuousTable))

ggplot(data = melt(continuousTable), mapping = aes(x = value)) + 
  geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free', ncol = 4) + 
  labs(title="Faceted Histogrms for Continuous Variabls", title_x="", title_y="")

 
```

<!--
# ```{r, echo=TRUE}
# 
# # descriptive statistics, load into new data frame for processing
# descriptiveTable <- stat.desc(employeeDatRaw)
# 
# # remove non-numeric features
# dropcolumns <- c("Attrition", "BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "Over18", "OverTime")
# descriptiveTable <- descriptiveTable[,!(colnames(descriptiveTable) %in% dropcolumns)]
# 
# # round all numeric values to 2 decimal points
# descriptiveTable <- round(descriptiveTable, 2)
# 
# # remove rows for statistics we don't care about, leaving: N, Mean, Median, Std Dev, Var, Min, Max
# remove <- c("CI.mean.0.95", "nbr.val", "nbr.null", "nbr.na", "range", "sum", "SE.mean", "CI.mean", "coef.var")
# descriptiveTable <- descriptiveTable[-which(rownames(descriptiveTable) %in% remove),]
# 
# # display descriptive statistics
# knitr::kable(descriptiveTable,caption = "Descriptive Statistics for Numeric Features in the Raw Employee Data", row.names = TRUE, "html") %>%
#   kable_styling(bootstrap_options = c("striped","hover", "condensed", "responsive"))
# 
# ```
-->

# Principal Components Analysis

```{r, echo=TRUE}
# remove non-numeric features, and features with zero variance
dropcolumnsPCA <- c("Attrition", "BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "Over18", "OverTime", "EmployeeCount", "StandardHours")
employeeDatPCA <- employeeDatRaw[,!(colnames(employeeDatRaw) %in% dropcolumnsPCA)]

# perform PCA
employee_PCA <- prcomp(employeeDatPCA, center=TRUE, scale.=TRUE)

# print principal components (only the first 4 PCs)
print(employee_PCA$rotation[,1:4])

# summary of principal components
summary(employee_PCA)
```

```{r, echo=TRUE, fig.width=15, fig.height=10}
# plot PCs using ggbiplot
ggbiplot(employee_PCA, obs.scale=1, var.scale=1, groups=employeeDatPCA$Attrition, ellipse=TRUE, circle=TRUE) + theme(legend.position='top') + scale_color_discrete()

biplot(employee_PCA) # Base R
```

# Exploring Attrition with Binomial Logistic Regression

Binomial logistic regression is a special form of mutiple regression that is used to model a dichotomous outcome. In our case, this outcome is whether an employee left the company or is still a current employee.

The executive leadership has identified predicting employee turnover as a primary application of data science for talent management. We will use binomial logistic regression to understand how certain explanatory variables may influence the likelihood of empoyee attrition. We will also identify a prediction model using explanatory variables in the data provided. This model can be used to identify high risk individuals to prioritize corrective action to improve employee attrition.

The first model will use all available continuous and categorical variables - that is, fitting a full model. Some variables must be left out as they do not have any variability and will cause the model fitting to error. These varables are: EmployeeCount, StandardHours, and Over18. For more information on these variables please review the Exploratory Data Analysis section.

The model will be fitted on 80% of the data selected at random from the raw data. The remaining 20% will be used to assess the prediction capability.

```{r, echo=TRUE, fig.width=6}

# copy data set, we can remove features with the dropcolumns as needed to play with the model
# I'm dropping variables with no variation per this 
# https://stackoverflow.com/questions/18171246/error-in-contrasts-when-defining-a-linear-model-in-r
# Solution: There is not enough variation in dependent variable with only one value. So, you need to drop that variable, irrespective of whether that is numeric or character or factor variable.

employee_logistic <- employeeDatRaw
dropcolumns <- c("EmployeeCount", "StandardHours", "Over18")
employee_logistic <- employee_logistic[,!(colnames(employee_logistic) %in% dropcolumns)]

# convert Attrition to 1 and 0 from character based factor
# convert factor levels to numeric
employee_logistic$Attrition <- as.numeric(as.factor(employee_logistic$Attrition))
# change the 2's (No) to 0's
employee_logistic$Attrition[employee_logistic$Attrition == 2] <- 0
# convert back to factor
employee_logistic$Attrition <- as.factor(employee_logistic$Attrition)

# split the raw data into testing and training data
set.seed(50) # set seed so that same sample can be reproduced in future

# now selecting 80% of data as sample from total 'n' rows of the data  
sample <- sample.int(n=nrow(employee_logistic), size=floor(.80*nrow(employee_logistic)), replace=FALSE)

# subset the data using the sample integer vector created above
train <- employee_logistic[sample, ]
test  <- employee_logistic[-sample, ]

# fitting the binomial logistic regression model, Attrition is dependent, fitting using all features
model <- glm(train$Attrition ~., family=binomial(link='logit'),data=train)

summary(model)
```

We will now test the predictive capablity of this full model.

```{r, echo=TRUE}
# predict based on the test data, type='response' output probabilities in the form of P(y=1|X)
fittedresults <- predict(model, newdata=test, type='response')

# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
fittedresults <- ifelse(fittedresults > 0.5, 1, 0)

# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(fittedresults != test$Attrition, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)

# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))
```

The model already exhibits very high predictive capability (86.4%), but we will now refit the model using only variables with signficance from the full model. This done to simplify the model for interpretation and to reduce potential multicolinearity issues.

The model will be fit with the following features:

BusinessTravel
DistanceFromHome
EnvironmentSatisfaction
Gender
JobInvolvement
JobRole (??)
JobSatisfaction
MaritalStatus
NumCompaniesWorked
OverTime
RelationshipSatisfaction
TotalWorkingYears
TrainingTimesLastYear
WorkLifeBalance
YearsAtCompany
YearsInCurrentRole
YearsSinceLastPromotion
YearsWithCurrManager

```{r, echo=TRUE}
employee_logistic <- employeeDatRaw
dropcolumns <- c("EmployeeCount", "StandardHours", "Over18") # revmoving these again, because they can't be used in the model
employee_logistic <- employee_logistic[,!(colnames(employee_logistic) %in% dropcolumns)]

# keep only features as noted above (and Attrition for predictions)
keepcolumns <- c("BusinessTravel", "DistanceFromHome", "EnvironmentSatisfaction",
"Gender", "JobInvolvement", "JobRole", "JobSatisfaction", "MaritalStatus",
"NumCompaniesWorked", "OverTime", "RelationshipSatisfaction",
"TotalWorkingYears", "TrainingTimesLastYear", "WorkLifeBalance",
"YearsAtCompany", "YearsInCurrentRole", "YearsSinceLastPromotion",
"YearsWithCurrManager", "Attrition")
employee_logistic <- employee_logistic[,(colnames(employee_logistic) %in% keepcolumns)]

# convert Attrition to 1 and 0 from character based factor
# convert factor levels to numeric
employee_logistic$Attrition <- as.numeric(as.factor(employee_logistic$Attrition))
# change the 2's (No) to 0's
employee_logistic$Attrition[employee_logistic$Attrition == 2] <- 0
# convert back to factor
employee_logistic$Attrition <- as.factor(employee_logistic$Attrition)

# split the raw data into testing and training data
set.seed(50) # set seed so that same sample can be reproduced in future

# now selecting 80% of data as sample from total 'n' rows of the data  
sample <- sample.int(n=nrow(employee_logistic), size=floor(.80*nrow(employee_logistic)), replace=FALSE)

# subset the data using the sample integer vector created above
train <- employee_logistic[sample, ]
test  <- employee_logistic[-sample, ]

# fitting the binomial logistic regression model, Attrition is dependent, fitting using all features
model <- glm(train$Attrition ~., family=binomial(link='logit'),data=train)

summary(model)
```

We will now test the predictive capablity of this reduced model.

```{r, echo=TRUE}
# predict based on the test data, type='response' output probabilities in the form of P(y=1|X)
fittedresults <- predict(model, newdata=test, type='response')

# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
fittedresults <- ifelse(fittedresults > 0.5, 1, 0)

# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(fittedresults != test$Attrition, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)

# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))
```

The predictive capability of this reduced model improved slightly and has now been simplifed quite a bit in terms of the number of features.






# Using GLMNET for Feature Selection of Logistic Regression
```{r, echo=TRUE}
employee_logistic <- employeeDatRaw
dropcolumns <- c("EmployeeCount", "StandardHours", "Over18") # removing these again, because they can't be used in the model
employee_logistic <- employee_logistic[,!(colnames(employee_logistic) %in% dropcolumns)]

# convert Attrition to 1 and 0 from character based factor
# convert factor levels to numeric
employee_logistic$Attrition <- as.numeric(as.factor(employee_logistic$Attrition))
# change the 2's (No) to 0's
employee_logistic$Attrition[employee_logistic$Attrition == 2] <- 0
# convert back to factor
employee_logistic$Attrition <- as.factor(employee_logistic$Attrition)

# split the raw data into testing and training data
set.seed(50) # set seed so that same sample can be reproduced in future

# now selecting 80% of data as sample from total 'n' rows of the data  
sample <- sample.int(n=nrow(employee_logistic), size=floor(.80*nrow(employee_logistic)), replace=FALSE)

# subset the data using the sample integer vector created above
train <- employee_logistic[sample, ]
test  <- employee_logistic[-sample, ]

# # # START GLMNET for Logistic Regression Feature Selection # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

# isolate the binary response "Attrition" from the training data
GLMTrain.y <- train$Attrition
GLMTrain.y <- as.factor(as.character(GLMTrain.y))

# create train data set while removing "Attrition" from the training data
GLMTrain.x <- train[,!(colnames(train) == "Attrition")]

#Categorical variables are usually first transformed into factors, then a dummy variable matrix of predictors is created and along with the continuous predictors, is passed to the model. Keep in mind, glmnet uses both ridge and lasso penalties, but can be set to either alone.

# isolate categorical/factors from the continuous features, create dummy variable matrix for all factors
GLMTrain.xfactors <- model.matrix(GLMTrain.y ~ GLMTrain.x$BusinessTravel + GLMTrain.x$Department + GLMTrain.x$EducationField + GLMTrain.x$Gender + GLMTrain.x$JobRole + GLMTrain.x$MaritalStatus + GLMTrain.x$OverTime)[, -1]

# remove categorical/factors from GLMTrain.x as they will be added back in the form of dummy variable matrix from above
dropcolumns <- c("BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "OverTime") 
GLMTrain.x <- GLMTrain.x[,!(colnames(GLMTrain.x) %in% dropcolumns)]

# combine GLMTrain.x continuous variables with GLMTrain.xfactors dummy variable matrix, then converting whole thing to a matrix for glmnet
GLMTrain.x <- as.matrix(data.frame(GLMTrain.x, GLMTrain.xfactors))

# use glmnet to fit a binomial logistic regression
glmnetfit <- glmnet(GLMTrain.x, GLMTrain.y, family = "binomial")

plot(glmnetfit)

# # # START Prediction from GLMNET fit model # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

# prepare the test data in a similar manner for GLMNET usage (create dummy variables, matrix, etc.)
# create test data set while removing "Attrition" from the test data
GLMTest.x <- test[,!(colnames(test) == "Attrition")]

# isolate the binary response "Attrition" from the test data
GLMTest.y <- test$Attrition
GLMTest.y <- as.factor(as.character(GLMTest.y))

# isolate categorical/factors from the continuous features, create dummy variable matrix for all factors
GLMTest.xfactors <- model.matrix(GLMTest.y ~ GLMTest.x$BusinessTravel + GLMTest.x$Department + GLMTest.x$EducationField + GLMTest.x$Gender + GLMTest.x$JobRole + GLMTest.x$MaritalStatus + GLMTest.x$OverTime)[, -1]

# remove categorical/factors from GLMTest.x as they will be added back in the form of dummy variable matrix from above
dropcolumns <- c("BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "OverTime") 
GLMTest.x <- GLMTest.x[,!(colnames(GLMTest.x) %in% dropcolumns)]

# combine GLMTest.x continuous variables with GLMTest.xfactors dummy variable matrix, then converting whole thing to a matrix for glmnet
GLMTest.x <- as.matrix(data.frame(GLMTest.x, GLMTest.xfactors))

# predict based on the test data, type='response' output probabilities in the form of P(y=1|X)
GLMfittedresults <- predict(glmnetfit, newx=GLMTest.x, type='response')

# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
GLMfittedresults <- ifelse(GLMfittedresults > 0.5, 1, 0)

# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(GLMfittedresults != GLMTest.y, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)

# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))

```












and now some ROC curves...

```{r, echo=TRUE}
#Create ROC curves
pr <- prediction(fittedresults, test$Attrition)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

#Ref line indicating poor performance, 50/50
abline(a=0, b= 1)

# calculate area under curve (AUC)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]

# print AUC onto plot
text(x = .40, y = .6,paste("AUC = ", round(auc,3), sep = ""))




