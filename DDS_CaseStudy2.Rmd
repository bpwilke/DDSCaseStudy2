---
title: "DDSCaseStudy2"
author: "Benjamin Wilke, Manny Rosales, Amy Paschal"
date: "April 2, 2018"
output:
  html_document:
    keep_md: true
---
```{r setup, include=FALSE}
# Load necessary packages and ensure they are active
load.lib = c("kableExtra","ggplot2","Amelia","pastecs","ROCR","reshape2","devtools")

install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
  install.packages(lib,dependences=TRUE)
} 

ggbi <- "ggbiplot"
if (!ggbi %in% installed.packages()){
  install_github("vqv/ggbiplot") # Not on CRAN, must install via GitHub using devtools package
} 

load.lib = c(load.lib,"ggbiplot")
sapply(load.lib,require,character=TRUE)

knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(kableExtra) # to make uber sexy tables for output
library(ggbiplot) # implementation of biplot using ggplot2 for plotting PCs
library(pastecs) # for easy descriptive statistics
library(ROCR) # for ROC plots and AUC calculations

library(glmnet)
# turn off scientfic notation for the entire script
options(scipen = 999, digits=2)
```

## Introduction

```{r, echo=TRUE}
# load raw data
employeeDatRaw <- read.csv('CaseStudy2data.csv', header=TRUE)
```

# Missing Values

Fortunately, this data is not missing any values as demonstrated by the table below.

```{r, echo=TRUE}
# calculate NAs for raw data
NAdataframe <- sapply(employeeDatRaw, function(x) sum(is.na(x)))
# display NAs in kable table
knitr::kable(NAdataframe, caption = "Counts of NAs (Missing Values) in Raw Data", row.names = TRUE, "html") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"), full_width=FALSE, position = "left")
```

## Data Cleanup and Conversion

#### Checking for missing data values . . .

```{r, echo=TRUE}
# check for missing data values
numNAs <- sum(apply(employeeDatRaw,2,is.na))
```

##### Number of missing data values = `r numNAs`

#### Columns with no variation have no impact on attrition. Dropping . . .

```{r, echo=TRUE}

# look for and drop columns with no variation
drop_columns <- which(apply(employeeDatRaw, 2, function(x) (length(unique(x)) == 1)))
cols <- names(drop_columns)
employeeDatRaw <- employeeDatRaw[,-drop_columns]

# find columns of class factor
factor_columns <- names(which(sapply(names(employeeDatRaw),function(x) class(employeeDatRaw[[x]])=="factor")))

# convert factors to numeric
# employeeDatRaw[,factor_columns] <- sapply(factor_columns,function(x) as.numeric(employeeDatRaw[[x]])-1)
```

##### Dropped columns: `r cols`

```{r, echo=TRUE}
employeeDatRaw$Attrition <- as.numeric(employeeDatRaw$Attrition)-1

# determine overall attrition rate
attritionRate <- (sum(employeeDatRaw$Attrition) / nrow(employeeDatRaw)) * 100
```

## Attrition Rates (Competitor Analysis)

##### Overall attrition rate for XYZ Company: `r attritionRate`%
### Per Compdata Surveys & Consulting: Turnover Report 2017
#### Attrition rates in 2017 for all industires: 13.5% voluntary and 18.5% overall.
#### Attrition rates in 2017 for Healthcare: 15.9% voluntary and 20.5% overall.
#### Attrition rates in 2017 for Manufacturing and Distribution: 11.1% voluntary and 17.0% overall.


## Additional Information to Collect
##### Voluntary/involuntary breakdown
##### Better job/department breakdown for Research & Development

```{r, echo=TRUE}
# determine attrition rate by department
AttritionByDept <- aggregate(employeeDatRaw$Attrition,by=list(Department=employeeDatRaw$Department),FUN=sum)
SizeByDept <- count(employeeDatRaw,"Department")
attritionRateByDept <- merge(AttritionByDept, SizeByDept, by="Department")
attritionRateByDept$Rate <- (attritionRateByDept$x / attritionRateByDept$freq) * 100
names(attritionRateByDept) <- c("Department", "Attrition", "PopulationSize", "AttritionRate")

# display attrition by department
knitr::kable(attritionRateByDept,caption = "Attrition Rates by Department", row.names = TRUE, "html") %>%
  kable_styling(bootstrap_options = c("striped","hover", "condensed", "responsive"))

# determine attrition rate by job role
attritionByJobRole <- aggregate(employeeDatRaw$Attrition, by=list(JobRole=employeeDatRaw$JobRole), FUN=sum)
names(attritionByJobRole) <- c("JobRole","Attrition")
satisfactionByJobRole <- aggregate(employeeDatRaw$JobSatisfaction, by=list(JobRole=employeeDatRaw$JobRole), FUN=stat.desc)
# sizeByJobRole <- count(employeeDatRaw,"JobRole")
attritionRateByJobRole <- cbind(attritionByJobRole, satisfactionByJobRole$x)
attritionRateByJobRole$AttritionRate <- (attritionRateByJobRole$Attrition / attritionRateByJobRole$nbr.val) * 100
drop_columns <- c("nbr.null","nbr.na","range","sum","SE.mean","CI.mean.0.95","coef.var")
attritionRateByJobRole <- attritionRateByJobRole[ , !(names(attritionRateByJobRole) %in% drop_columns)]
 
names(attritionRateByJobRole) <- c("JobRole", "Attrition", "PopulationSize", "SatMin", "SatMax", "SatMedian","SatMean","SatVar","SatStdDev","AttritionRate")


# display attrition rate by job role
knitr::kable(attritionRateByJobRole,caption = "Attrition Rates by Job Role, Including Job Satisfaction Statistics", row.names = TRUE, "html") %>%
  kable_styling(bootstrap_options = c("striped","hover", "condensed", "responsive"))

```


# Exploratory Data Analysis

From the data provided we can clearly see that there is a subset of variabes that are continuous and another that are categorical. In order to better understand the distribution and skew of the continuous numerical columns a set of faceted histograms was generated. Other than Age which appears normaly distributed, it is clear that most columns are right skewed. Each factor for the categorical columns is summarized in a review. One of the more interesting distributions include YearsSinceLastPromotion, the large right skew could indicate a significant factor associated with attrition. 

We also notice from the data that EmployeeCount and StandardHours are the same value for all employees and are not meaningful variables. Similarly, EmployeeNumber does not produce any meaingful information given that each employee has a unique EmployeeNumber.

When incorporating Attrition into the histograms it appears that those with more YearsAtCompany are less likely to leave the company. A similar trend is obsered with MonthlyIncome, the larger the MonthlyIncome the less Attrition is observed. Finally those with a higher JobLevel also appear to have less attrition than those with lower JobLevel values.

``` {r MannyEDA, echo=TRUE,fig.height = 16, fig.width = 10, fig.align="center"}
# descriptive statistics, load into new data frame for processing
descriptiveTable <- pastecs::stat.desc(employeeDatRaw)

# remove non-numeric features
charCols <- c("Attrition", "BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "Over18", "OverTime")
continuousTable.stats <- descriptiveTable[,!(colnames(descriptiveTable) %in% charCols)]
categoricalTable <- employeeDatRaw[,(colnames(descriptiveTable) %in% charCols)]
continuousTable <- employeeDatRaw[,!(colnames(employeeDatRaw) %in% charCols)]

# remove rows for certain descriptive statistics leaving: N, Mean, Median, Std Dev, Var, Min, Max
remove <- c("CI.mean.0.95", "nbr.val", "nbr.null", "nbr.na", "range", "sum", "SE.mean", "CI.mean", "coef.var")
continuousTable.stats <- continuousTable.stats[-which(rownames(continuousTable.stats) %in% remove),]

# round all numeric values to 2 decimal points
continuousTable.stats <- round(continuousTable.stats, 2)
continuousTable.transposed <- t(continuousTable.stats) # object becomes matrix

# display descriptive statistics
knitr::kable(continuousTable.transposed,caption = "Descriptive Statistics for Numeric Features in the Raw Employee Data", row.names = TRUE, "html") %>%
  kable_styling(bootstrap_options = c("striped","hover", "condensed", "responsive"), full_width = F)

summary(categoricalTable)

continuousTable$Attrition <- employeeDatRaw$Attrition
continuousTable$EmployeeCount <- NULL
continuousTable$EmployeeNumber <- NULL
continuousTable$StandardHours <- NULL

facetPlot <- melt(continuousTable, id.vars = "Attrition")

p <- ggplot(data = facetPlot, aes(x = value, fill=Attrition)) + 
    geom_histogram(bins = 10, colour = "black") + 
    facet_wrap(~variable, scales = 'free', ncol = 4) + 
    labs(title="Faceted Histograms for Continuous Variables", title_x="", title_y="") +
    scale_fill_manual(values = c("darkgrey","red")) 
    
p  
```
## rmd not showing correct image, below just showing an export as a backup

![Backup](https://github.com/bpwilke/DDSCaseStudy2/blob/MannyWorking2/DDS_CaseStudy2_files/ImageBackup.png)

<!--
# ```{r, echo=TRUE}
# 
# # descriptive statistics, load into new data frame for processing
# descriptiveTable <- stat.desc(employeeDatRaw)
# 
# # remove non-numeric features
# dropcolumns <- c("Attrition", "BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "Over18", "OverTime")
# descriptiveTable <- descriptiveTable[,!(colnames(descriptiveTable) %in% dropcolumns)]
# 
# # round all numeric values to 2 decimal points
# descriptiveTable <- round(descriptiveTable, 2)
# 
# # remove rows for statistics we don't care about, leaving: N, Mean, Median, Std Dev, Var, Min, Max
# remove <- c("CI.mean.0.95", "nbr.val", "nbr.null", "nbr.na", "range", "sum", "SE.mean", "CI.mean", "coef.var")
# descriptiveTable <- descriptiveTable[-which(rownames(descriptiveTable) %in% remove),]
# 
# # display descriptive statistics
# knitr::kable(descriptiveTable,caption = "Descriptive Statistics for Numeric Features in the Raw Employee Data", row.names = TRUE, "html") %>%
#   kable_styling(bootstrap_options = c("striped","hover", "condensed", "responsive"))
# 
# ```
-->

# Principal Components Analysis

```{r, echo=TRUE}
# remove non-numeric features, and features with zero variance
dropcolumnsPCA <- c("Attrition", "BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "Over18", "OverTime", "EmployeeCount", "StandardHours")
employeeDatPCA <- employeeDatRaw[,!(colnames(employeeDatRaw) %in% dropcolumnsPCA)]

# perform PCA
employee_PCA <- prcomp(employeeDatPCA, center=TRUE, scale.=TRUE)

# print principal components (only the first 4 PCs)
print(employee_PCA$rotation[,1:4])

# summary of principal components
summary(employee_PCA)
```

```{r, echo=TRUE, fig.width=15, fig.height=10}
# plot PCs using ggbiplot
ggbiplot(employee_PCA, obs.scale=1, var.scale=1, groups=employeeDatPCA$Attrition, ellipse=TRUE, circle=TRUE) + theme(legend.position='top') + scale_color_discrete()

biplot(employee_PCA) # Base R
```

# Exploring Attrition with Binomial Logistic Regression

Binomial logistic regression is a special form of mutiple regression that is used to model a dichotomous outcome. In our case, this outcome is whether an employee left the company or is still a current employee.

The executive leadership has identified predicting employee turnover as a primary application of data science for talent management. We will use binomial logistic regression to understand how certain explanatory variables may influence the likelihood of empoyee attrition. We will also identify a prediction model using explanatory variables in the data provided. This model can be used to identify high risk individuals to prioritize corrective action to improve employee attrition.

The first model will use all available continuous and categorical variables - that is, fitting a full model. Some variables must be left out as they do not have any variability and will cause the model fitting to error. These varables are: EmployeeCount, StandardHours, and Over18. For more information on these variables please review the Exploratory Data Analysis section.

The model will be fitted on 80% of the data selected at random from the raw data. The remaining 20% will be used to assess the prediction capability.

```{r, echo=TRUE, fig.width=6}

# copy data set, we can remove features with the dropcolumns as needed to play with the model
# I'm dropping variables with no variation per this 
# https://stackoverflow.com/questions/18171246/error-in-contrasts-when-defining-a-linear-model-in-r
# Solution: There is not enough variation in dependent variable with only one value. So, you need to drop that variable, irrespective of whether that is numeric or character or factor variable.

employee_logistic <- employeeDatRaw
dropcolumns <- c("EmployeeCount", "StandardHours", "Over18")
employee_logistic <- employee_logistic[,!(colnames(employee_logistic) %in% dropcolumns)]

# convert Attrition to 1 and 0 from character based factor
# convert factor levels to numeric
employee_logistic$Attrition <- as.numeric(as.factor(employee_logistic$Attrition))
# change the 2's (No) to 0's
employee_logistic$Attrition[employee_logistic$Attrition == 2] <- 0
# convert back to factor
employee_logistic$Attrition <- as.factor(employee_logistic$Attrition)

# split the raw data into testing and training data
set.seed(50) # set seed so that same sample can be reproduced in future

# now selecting 80% of data as sample from total 'n' rows of the data  
sample <- sample.int(n=nrow(employee_logistic), size=floor(.80*nrow(employee_logistic)), replace=FALSE)

# subset the data using the sample integer vector created above
train <- employee_logistic[sample, ]
test  <- employee_logistic[-sample, ]

# fitting the binomial logistic regression model, Attrition is dependent, fitting using all features
model <- glm(train$Attrition ~., family=binomial(link='logit'),data=train)

summary(model)
```

We will now test the predictive capablity of this full model.

```{r, echo=TRUE}
# predict based on the test data, type='response' output probabilities in the form of P(y=1|X)
fittedresults <- predict(model, newdata=test, type='response')

# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
fittedresults <- ifelse(fittedresults > 0.5, 1, 0)

# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(fittedresults != test$Attrition, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)

# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))
```

The model already exhibits very high predictive capability (86.4%), but we will now refit the model using only variables with signficance from the full model. This done to simplify the model for interpretation and to reduce potential multicolinearity issues.

The model will be fit with the following features:

BusinessTravel
DistanceFromHome
EnvironmentSatisfaction
Gender
JobInvolvement
JobRole (??)
JobSatisfaction
MaritalStatus
NumCompaniesWorked
OverTime
RelationshipSatisfaction
TotalWorkingYears
TrainingTimesLastYear
WorkLifeBalance
YearsAtCompany
YearsInCurrentRole
YearsSinceLastPromotion
YearsWithCurrManager

```{r, echo=TRUE}
employee_logistic <- employeeDatRaw
dropcolumns <- c("EmployeeCount", "StandardHours", "Over18") # revmoving these again, because they can't be used in the model
employee_logistic <- employee_logistic[,!(colnames(employee_logistic) %in% dropcolumns)]

# keep only features as noted above (and Attrition for predictions)
keepcolumns <- c("BusinessTravel", "DistanceFromHome", "EnvironmentSatisfaction",
"Gender", "JobInvolvement", "JobRole", "JobSatisfaction", "MaritalStatus",
"NumCompaniesWorked", "OverTime", "RelationshipSatisfaction",
"TotalWorkingYears", "TrainingTimesLastYear", "WorkLifeBalance",
"YearsAtCompany", "YearsInCurrentRole", "YearsSinceLastPromotion",
"YearsWithCurrManager", "Attrition")
employee_logistic <- employee_logistic[,(colnames(employee_logistic) %in% keepcolumns)]

# convert Attrition to 1 and 0 from character based factor
# convert factor levels to numeric
employee_logistic$Attrition <- as.numeric(as.factor(employee_logistic$Attrition))
# change the 2's (No) to 0's
employee_logistic$Attrition[employee_logistic$Attrition == 2] <- 0
# convert back to factor
employee_logistic$Attrition <- as.factor(employee_logistic$Attrition)

# split the raw data into testing and training data
set.seed(50) # set seed so that same sample can be reproduced in future

# now selecting 80% of data as sample from total 'n' rows of the data  
sample <- sample.int(n=nrow(employee_logistic), size=floor(.80*nrow(employee_logistic)), replace=FALSE)

# subset the data using the sample integer vector created above
train <- employee_logistic[sample, ]
test  <- employee_logistic[-sample, ]

# fitting the binomial logistic regression model, Attrition is dependent, fitting using all features
model <- glm(train$Attrition ~., family=binomial(link='logit'),data=train)

summary(model)
```

We will now test the predictive capablity of this reduced model.

```{r, echo=TRUE}
# predict based on the test data, type='response' output probabilities in the form of P(y=1|X)
fittedresults <- predict(model, newdata=test, type='response')

# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
fittedresults <- ifelse(fittedresults > 0.5, 1, 0)

# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(fittedresults != test$Attrition, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)

# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))
```

The predictive capability of this reduced model improved slightly and has now been simplifed quite a bit in terms of the number of features.






# Using GLMNET for Feature Selection of Logistic Regression
```{r, echo=TRUE}
employee_logistic <- employeeDatRaw
dropcolumns <- c("EmployeeCount", "StandardHours", "Over18") # removing these again, because they can't be used in the model
employee_logistic <- employee_logistic[,!(colnames(employee_logistic) %in% dropcolumns)]

# convert Attrition to 1 and 0 from character based factor
# convert factor levels to numeric
employee_logistic$Attrition <- as.numeric(as.factor(employee_logistic$Attrition))
# change the 2's (No) to 0's
employee_logistic$Attrition[employee_logistic$Attrition == 2] <- 0
# convert back to factor
employee_logistic$Attrition <- as.factor(employee_logistic$Attrition)

# split the raw data into testing and training data
set.seed(50) # set seed so that same sample can be reproduced in future

# now selecting 80% of data as sample from total 'n' rows of the data  
sample <- sample.int(n=nrow(employee_logistic), size=floor(.80*nrow(employee_logistic)), replace=FALSE)

# subset the data using the sample integer vector created above
train <- employee_logistic[sample, ]
test  <- employee_logistic[-sample, ]

# # # START GLMNET for Logistic Regression Feature Selection # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

# isolate the binary response "Attrition" from the training data
GLMTrain.y <- train$Attrition
GLMTrain.y <- as.factor(as.character(GLMTrain.y))

# create train data set while removing "Attrition" from the training data
GLMTrain.x <- train[,!(colnames(train) == "Attrition")]

#Categorical variables are usually first transformed into factors, then a dummy variable matrix of predictors is created and along with the continuous predictors, is passed to the model. Keep in mind, glmnet uses both ridge and lasso penalties, but can be set to either alone.

# isolate categorical/factors from the continuous features, create dummy variable matrix for all factors
GLMTrain.xfactors <- model.matrix(GLMTrain.y ~ GLMTrain.x$BusinessTravel + GLMTrain.x$Department + GLMTrain.x$EducationField + GLMTrain.x$Gender + GLMTrain.x$JobRole + GLMTrain.x$MaritalStatus + GLMTrain.x$OverTime)[, -1]

# remove categorical/factors from GLMTrain.x as they will be added back in the form of dummy variable matrix from above
dropcolumns <- c("BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "OverTime") 
GLMTrain.x <- GLMTrain.x[,!(colnames(GLMTrain.x) %in% dropcolumns)]

# combine GLMTrain.x continuous variables with GLMTrain.xfactors dummy variable matrix, then converting whole thing to a matrix for glmnet
GLMTrain.x <- as.matrix(data.frame(GLMTrain.x, GLMTrain.xfactors))

# use glmnet to fit a binomial logistic regression
glmnetfit <- glmnet(GLMTrain.x, GLMTrain.y, family = "binomial")

plot(glmnetfit)

# # # START Prediction from GLMNET fit model # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

# prepare the test data in a similar manner for GLMNET usage (create dummy variables, matrix, etc.)
# create test data set while removing "Attrition" from the test data
GLMTest.x <- test[,!(colnames(test) == "Attrition")]

# isolate the binary response "Attrition" from the test data
GLMTest.y <- test$Attrition
GLMTest.y <- as.factor(as.character(GLMTest.y))

# isolate categorical/factors from the continuous features, create dummy variable matrix for all factors
GLMTest.xfactors <- model.matrix(GLMTest.y ~ GLMTest.x$BusinessTravel + GLMTest.x$Department + GLMTest.x$EducationField + GLMTest.x$Gender + GLMTest.x$JobRole + GLMTest.x$MaritalStatus + GLMTest.x$OverTime)[, -1]

# remove categorical/factors from GLMTest.x as they will be added back in the form of dummy variable matrix from above
dropcolumns <- c("BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "OverTime") 
GLMTest.x <- GLMTest.x[,!(colnames(GLMTest.x) %in% dropcolumns)]

# combine GLMTest.x continuous variables with GLMTest.xfactors dummy variable matrix, then converting whole thing to a matrix for glmnet
GLMTest.x <- as.matrix(data.frame(GLMTest.x, GLMTest.xfactors))

# predict based on the test data, type='response' output probabilities in the form of P(y=1|X)
GLMfittedresults <- predict(glmnetfit, newx=GLMTest.x, type='response')

# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
GLMfittedresults <- ifelse(GLMfittedresults > 0.5, 1, 0)

# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(GLMfittedresults != GLMTest.y, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)

# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))

```












and now some ROC curves...

```{r, echo=TRUE}
#Create ROC curves
pr <- prediction(fittedresults, test$Attrition)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

#Ref line indicating poor performance, 50/50
abline(a=0, b= 1)

# calculate area under curve (AUC)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]

# print AUC onto plot
text(x = .40, y = .6,paste("AUC = ", round(auc,3), sep = ""))




